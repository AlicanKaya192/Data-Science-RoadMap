MÜLAKAT SORULARI

1. Teknik olarak kendinizi geliştirmeniz gerektiğini düşündüğünüz alanlarınızdan bahseder misiniz?

2. Bize biraz kendinizden bahsedip aşağıdaki ilanı göz önüne alarak bu pozisyon için neden uygun olduğunuzu düşündüğünüzü anlatabilir misiniz?

   Nitelikler
   - Python kullanarak veri analizi, veri manipülasyonu ve modelleme bilgisi olan,
   - Temel seviyede SQL bilen,
   - Makine Öğrenmesi teknikleri konusunda bilgi sahibi,
   - Kapsamlı A/B testi metodolojileri uygulayarak kampanyaların etkisini ve getirisini ölçme,
   - Problem çözme kabiliyeti olan,
   - Analitik düşünebilen, sorumluluk sahibi ve sonuç odaklı,
   - Takım Çalışmasına uygun, insan ilişkileri ve iletişim yönü güçlü,
   - Planlama ve zaman yönetimi konusunda bilgi sahibi

   İş Tanımı
   - İlgili birimlerle çalışarak veri gereksinimlerini anlamak,
   - Çeşitli makine öğrenmesi tekniklerini kullanarak tahmine dayalı modeller geliştirmek,
   - Uçtan uca proje geliştirme süreçlerinde yer almak (ihtiyacı anlama, veriyi anlama, veriyi hazırlama, modelleme, sonuçların değerlendirilmesi)


3. Maaş beklentinizi paylaşır mısınız?

   "Veri bilimi alanındaki eğitimim ve projelerim sayesinde temel istatistik, Python, SQL ve veri görselleştirme konularında güçlü bir altyapıya sahibim.
   Junior bir veri bilimci olarak, öğrenmeye ve gelişmeye açık bir şekilde ekibe katkı sağlayabileceğimi düşünüyorum. Bu doğrultuda,
   Türkiye’deki junior veri bilimci maaş aralıklarını da göz önünde bulundurarak, aylık 50.000 TL – 65.000 TL aralığında bir maaş beklentim bulunmaktadır.
   Elbette, şirketin sunduğu fırsatlar ve yan hakları da değerlendirmeye açığım."

4. Geribildirim aldığınız bir durumdan örnek veriniz. Bu geribildirime nasıl karşılık verdiniz? Özellikle ne yaptınız ya da söylediniz? Sonuç ne oldu? Bu deneyimden ne öğrendiniz? Sonuç olarak neyi daha farklı yaptınız?

   "Geçmişte bir projede, ekip arkadaşlarımdan aldığım geribildirim üzerine iletişim tarzımı gözden geçirme fırsatı buldum.
   Proje sürecinde, bazı teknik terimleri ve kavramları ekip üyelerine yeterince açıklamadığımı fark ettim.
   Bu geribildirim üzerine, iletişimimi daha açık ve anlaşılır hale getirmek için çaba sarf ettim.
   Toplantılarda daha fazla soru sormaya ve ekip üyelerinin anlayış seviyesini gözlemlemeye başladım.
   Sonuç olarak, ekip içi iletişimimiz güçlendi ve projede daha verimli bir işbirliği sağlandı.
   Bu deneyimden, etkili iletişimin proje başarısı için kritik olduğunu öğrendim ve bu beceriyi sürekli geliştirmeye çalışıyorum."

5. Bir şeyin neden olduğuna dair bir dizi farklı kavram ya da hipotez geliştirdiğiniz ve test ettiğiniz bir durumu anlatınız. Durum neydi? Hipotezlerinizi nasıl geliştirdiniz ve test ettiniz? Yaklaşımınız neydi? Sonuç ne oldu?

   "Geçmişte, bir e-ticaret platformunda müşteri memnuniyetini artırmak için bir proje yürüttüm.
   İlk olarak, müşteri geri bildirimlerini analiz ederek memnuniyetsizlik nedenlerini belirledim.
   Hipotezlerimi geliştirmek için, müşteri hizmetleri yanıt süreleri, ürün kalitesi ve teslimat süreleri gibi faktörleri göz önünde bulundurdum.
   Bu hipotezleri test etmek için, A/B testi yöntemini kullandım; belirli müşteri gruplarına farklı hizmet iyileştirmeleri sundum ve sonuçları karşılaştırdım.
   Yaklaşımım, veri odaklı karar verme ve sürekli iyileştirme üzerine kuruluydu.
   Sonuç olarak, müşteri memnuniyetinde %15'lik bir artış sağladık ve bu deneyim bana hipotez geliştirme ve test etme süreçlerinde sistematik bir yaklaşımın önemini öğretti."

6. Aşırı öğrenme nedir?

   "Aşırı öğrenme (overfitting), bir makine öğrenmesi modelinin eğitim verisine çok iyi uyum sağlaması ancak yeni, görülmemiş verilere karşı zayıf performans göstermesi durumudur.
   Bu durum, modelin eğitim verisindeki gürültüyü ve rastgele dalgalanmaları da öğrenmesiyle ortaya çıkar.
   Aşırı öğrenme, modelin genelleme yeteneğini azaltır ve gerçek dünya uygulamalarında başarısız olmasına neden olabilir.
   Aşırı öğrenmeyi önlemek için çeşitli teknikler kullanılabilir, örneğin:
   - Daha fazla veri toplamak,
   - Model karmaşıklığını azaltmak (örneğin, daha basit modeller kullanmak),
   - Düzenlileştirme (regularization) yöntemleri uygulamak,
   - Erken durdurma (early stopping) tekniklerini kullanmak,
   - Çapraz doğrulama (cross-validation) yöntemleri ile model performansını değerlendirmek."

7. Model karmaşıklığı nedir?

   "Model karmaşıklığı, bir makine öğrenmesi modelinin yapısal özelliklerini ve kapasitesini ifade eder.
   Karmaşık modeller, daha fazla parametreye sahip olabilir ve eğitim verisindeki ayrıntıları daha iyi yakalayabilir.
   Ancak, aşırı karmaşık modeller aşırı öğrenmeye (overfitting) yol açabilir, bu da modelin yeni verilere karşı zayıf performans göstermesine neden olur.
   Öte yandan, çok basit modeller ise eğitim verisini yeterince temsil edemeyebilir ve düşük performans gösterebilir (underfitting).
   Model karmaşıklığını yönetmek için, model seçimi sırasında dikkatli olunmalı ve düzenlileştirme teknikleri gibi yöntemler kullanılmalıdır.
   Ayrıca, çapraz doğrulama gibi yöntemlerle modelin genelleme yeteneği değerlendirilmelidir."

8. Analitik çözüm ile optimizasyon çözümü arasındaki fark nedir?

   "Analitik çözüm, matematiksel yöntemler ve formüller kullanılarak bir problemin kesin ve kapalı bir şekilde çözülmesini ifade eder.
   Bu tür çözümler genellikle belirli varsayımlar altında geçerlidir ve genellikle tek bir doğru cevaba sahiptir.
   Örneğin, lineer denklemler sistemi analitik olarak çözülebilir.

   Optimizasyon çözümü ise, belirli bir hedefi maksimize etmek veya minimize etmek için en iyi çözümü bulmayı amaçlar.
   Optimizasyon problemleri genellikle kısıtlamalar içerir ve birçok olası çözüm arasından en iyisini seçmeyi gerektirir.
   Optimizasyon çözümleri genellikle yaklaşık çözümler olabilir ve çeşitli algoritmalar (örneğin, genetik algoritmalar, gradyan inişi) kullanılarak bulunabilir.

   Özetle, analitik çözümler kesin ve kapalı formda iken, optimizasyon çözümleri en iyi çözümü bulmaya yönelik süreçlerdir."

9. Çapraz doğrulama nedir açıklayınız. Model performansını artırır mı?

   "Çapraz doğrulama (cross-validation), bir makine öğrenmesi modelinin performansını değerlendirmek için kullanılan bir tekniktir.
   Bu yöntem, veri setini birkaç alt kümeye (folds) böler ve modelin her bir alt küme üzerinde eğitilip diğer alt kümeler üzerinde test edilmesini sağlar.
   En yaygın kullanılan çapraz doğrulama yöntemi k-katlı çapraz doğrulama (k-fold cross-validation) olup, veri seti k eşit parçaya bölünür.
   Model, k-1 parça üzerinde eğitilir ve kalan parça üzerinde test edilir; bu işlem k kez tekrarlanır ve sonuçlar ortalanır.

   Çapraz doğrulama, modelin genelleme yeteneğini daha iyi değerlendirmeye yardımcı olur ve aşırı öğrenmeyi (overfitting) tespit etmeye olanak tanır.
   Doğrudan model performansını artırmaz, ancak model seçimi ve hiperparametre ayarlaması süreçlerinde daha güvenilir sonuçlar elde edilmesini sağlar.
   Bu da dolaylı olarak model performansının iyileştirilmesine katkıda bulunabilir."

10. Makine öğrenmesi tekniklerinden regresyon ve sınıflandırmanın ortak özellikleri ve farkları nelerdir? 

   "Regresyon ve sınıflandırma, makine öğrenmesi alanında yaygın olarak kullanılan iki temel tekniktir ve her ikisi de denetimli öğrenme (supervised learning) kategorisine girer.

   Ortak Özellikler:
   - Her iki teknik de etiketli veri setleri kullanılarak model eğitimi yapar.
   - Her iki teknik de gelecekteki veriler üzerinde tahminler yapmak için kullanılır.
   - Her iki teknik de model performansını değerlendirmek için çeşitli metrikler kullanır.

   Farklar:
   - Regresyon, sürekli bir hedef değişkeni (örneğin, fiyat, sıcaklık) tahmin etmek için kullanılırken, sınıflandırma ise kategorik bir hedef değişkeni (örneğin, spam/ham, hastalık/sağlıklı) tahmin etmek için kullanılır.
   - Regresyon modelleri genellikle doğrusal regresyon, polinomial regresyon gibi teknikleri içerirken, sınıflandırma modelleri lojistik regresyon, karar ağaçları, destek vektör makineleri gibi teknikleri içerir.
   - Regresyon sonuçları genellikle gerçek sayılar iken, sınıflandırma sonuçları belirli sınıflar veya etiketlerdir.

   Özetle, regresyon sürekli değerler tahmin ederken, sınıflandırma belirli kategorilere ayırma işlemi yapar."

11. Yakın zamanda dahil olduğunuz bir takımdan bahsediniz. Takım ne idi? Bu takıma katılırken yaşadığınız önemli zorluklar nelerdi? Bunlarla nasıl başa çıktınız? İşyerinizdeki kişileri tanıma sürecinizde bu sizin için karakteristik bir özellik midir? Bu tecrübeden ne öğrendiniz?

   "Geçtiğimiz yıl bir veri bilimi projesi için yeni bir takıma katıldım. Takım, farklı disiplinlerden gelen üyelerden oluşuyordu ve bu çeşitlilik başlangıçta iletişim ve işbirliği açısından bazı zorluklar yarattı.
   Takıma katılırken, ekip üyelerinin çalışma tarzlarını ve beklentilerini anlamak için aktif bir şekilde dinlemeye ve gözlemlemeye odaklandım.
   Ayrıca, düzenli toplantılar düzenleyerek açık iletişimi teşvik ettim ve herkesin katkıda bulunabileceği bir ortam yaratmaya çalıştım.
   Bu süreçte, esneklik ve empati becerilerimi geliştirdim, bu da takım içindeki ilişkileri güçlendirmeme yardımcı oldu.
   Bu deneyim, yeni bir takıma katılırken adaptasyon sürecinin önemini ve etkili iletişimin takım başarısındaki kritik rolünü bana öğretti."

12. Random Forest’ı teknik olmayan birine nasıl anlatırsınız?

   "Random Forest, bir orman gibi düşünebileceğimiz birçok karar ağacından oluşan bir makine öğrenmesi modelidir.
   Her bir karar ağacı, verileri belirli kurallara göre sınıflandırır veya tahmin eder.
   Ancak, tek bir ağaç bazen hatalar yapabilir çünkü sadece belirli bir bakış açısına sahiptir.
   Random Forest, birçok farklı ağaç oluşturur ve her bir ağaç farklı verilerle eğitilir.
   Sonuç olarak, tüm ağaçların verdiği tahminler bir araya getirilir ve en yaygın olanı seçilir (sınıflandırma için) veya ortalaması alınır (regresyon için).
   Bu yöntem, tek bir ağacın yapabileceği hataları azaltır ve daha doğru sonuçlar elde etmemizi sağlar.
   Yani, Random Forest, birçok küçük karar ağacının birlikte çalışarak daha iyi kararlar vermesini sağlayan güçlü bir araçtır."

13. Gradient Boosting Machine(GBM) nedir? Nasıl çalışır?

   "Gradient Boosting Machine (GBM), makine öğrenmesi alanında kullanılan güçlü bir topluluk (ensemble) öğrenme yöntemidir.
   GBM, zayıf öğreniciler olarak adlandırılan basit modelleri (genellikle karar ağaçları) ardışık olarak eğitir ve her yeni model, önceki modellerin hatalarını düzeltmeye odaklanır.
   GBM'in çalışma prensibi şu şekildedir:
   1. İlk olarak, basit bir model (örneğin, küçük bir karar ağacı) eğitilir ve bu modelin tahminleri ile gerçek değerler arasındaki fark (hata) hesaplanır.
   2. Daha sonra, bu hataları minimize etmek için yeni bir model eğitilir. Bu yeni model, önceki modelin hatalarını tahmin etmeye çalışır.
   3. Bu süreç, belirli bir sayıda iterasyon veya hata belirli bir seviyeye inene kadar tekrarlanır.
   4. Sonuç olarak, tüm modellerin tahminleri toplanır ve nihai tahmin elde edilir.

   GBM, özellikle karmaşık veri setlerinde yüksek doğruluk sağlar ve çeşitli hiperparametrelerle (örneğin, öğrenme hızı, ağaç derinliği) ayarlanabilir.
   Ancak, aşırı öğrenme riskini azaltmak için dikkatli bir şekilde yapılandırılması gerekir."

14. Hiperparametre nedir? Ağaç yöntemleri için  en sık kullanılan hiperparametrelerden iki tanesini açıklayınız.

   "Hiperparametreler, makine öğrenmesi modellerinin eğitim sürecinden önce belirlenen ve modelin öğrenme sürecini kontrol eden parametrelerdir.
   Hiperparametreler, modelin performansını ve genelleme yeteneğini etkileyebilir ve genellikle model eğitimi sırasında optimize edilmezler.

   Ağaç yöntemleri için en sık kullanılan iki hiperparametre şunlardır:
   1. Maksimum Derinlik (max_depth): Bu hiperparametre, bir karar ağacının maksimum derinliğini belirler.
      Daha derin ağaçlar, veriye daha iyi uyum sağlayabilir ancak aşırı öğrenme riskini artırabilir.
      Bu nedenle, max_depth hiperparametresi, modelin karmaşıklığını kontrol etmek için kullanılır.

   2. Minimum Yaprak Düğüm Sayısı (min_samples_leaf): Bu hiperparametre, bir yaprak düğümünde bulunması gereken minimum örnek sayısını belirler.
      Daha yüksek bir değer, ağacın daha genel olmasını sağlar ve aşırı öğrenmeyi önlemeye yardımcı olabilir.
      Bu hiperparametre, modelin esnekliğini ve genelleme yeteneğini etkiler."

15. Sunumlarınızı farklı izleyiciler için nasıl değiştirirsiniz? (örneğin teknik geçmişi olan ve olmayan kişiler)

   "Sunumlarımı izleyicinin teknik geçmişine göre uyarlamak için öncelikle izleyicinin bilgi seviyesini ve ilgi alanlarını değerlendiririm.
   Teknik geçmişi olan izleyiciler için, daha derinlemesine teknik detaylara ve metodolojilere odaklanırım.
   Bu tür izleyicilere, kullanılan algoritmaların matematiksel temelleri, model performans metrikleri ve veri işleme teknikleri hakkında daha fazla bilgi sunarım.
   Ayrıca, kod örnekleri ve teknik terimler kullanmaktan çekinmem.

   Öte yandan, teknik geçmişi olmayan izleyiciler için sunumumu daha sade ve anlaşılır hale getiririm.
   Karmaşık teknik terimleri basit bir dille açıklarım ve görsel öğeler (grafikler, diyagramlar) kullanarak kavramları somutlaştırırım.
   Ayrıca, gerçek dünya uygulamaları ve iş sonuçlarına odaklanarak, izleyicinin ilgisini çekmeye çalışırım.
   Bu tür izleyicilere, modelin nasıl çalıştığını ve iş süreçlerine nasıl katkıda bulunduğunu vurgularım.

   Genel olarak, izleyici kitlesine uygun bir dil ve içerik seçmek, sunumun etkili ve anlaşılır olmasını sağlar."

16. PCA nedir? Hangi amaçlar için kullanılır?

   "PCA (Principal Component Analysis), çok boyutlu veri setlerinde boyut indirgeme amacıyla kullanılan bir istatistiksel tekniktir.
   PCA, orijinal değişkenler arasındaki korelasyonları analiz ederek, verinin varyansını en iyi şekilde temsil eden yeni, daha az sayıda değişken (ana bileşenler) oluşturur.
   Bu ana bileşenler, orijinal değişkenlerin doğrusal kombinasyonlarıdır ve birbirinden bağımsızdır.

   PCA'nın başlıca kullanım amaçları şunlardır:
   1. Boyut İndirgeme: Yüksek boyutlu veri setlerinde, gereksiz veya az bilgi içeren değişkenleri azaltarak veri setini daha yönetilebilir hale getirir.
   2. Görselleştirme: İki veya üç boyuta indirgenmiş veriler, görselleştirme için kullanılabilir, bu da veri yapısını ve ilişkilerini daha iyi anlamayı sağlar.
   3. Gürültü Azaltma: PCA, verideki gürültüyü azaltarak model performansını artırabilir.
   4. Özellik Çıkarımı: Yeni özellikler oluşturmak için kullanılabilir, bu da makine öğrenmesi modellerinin performansını iyileştirebilir.

   PCA, özellikle büyük veri setlerinde ve yüksek boyutlu verilerle çalışırken faydalıdır."

17. Unsupervised learning sürecinden supervised learning’e nasıl geçeriz?

   "Unsupervised learning (denetimsiz öğrenme) sürecinden supervised learning (denetimli öğrenme) sürecine geçiş yapmak için aşağıdaki adımlar izlenebilir:
   1. Veri Keşfi ve Ön İşleme: Unsupervised learning ile elde edilen veriler üzerinde keşif yaparak, veri setindeki yapıları ve desenleri anlamak önemlidir. Eksik verilerin doldurulması, aykırı değerlerin tespiti ve veri normalizasyonu gibi ön işleme adımları gerçekleştirilir.
   2. Kümeleme Sonuçlarının Etiketlenmesi: Unsupervised learning teknikleri (örneğin, K-means kümeleme) kullanılarak veriler gruplandırılır. Elde edilen kümeler, domain bilgisi veya uzman görüşü kullanılarak etiketlenir. Bu etiketler, supervised learning için hedef değişken olarak kullanılabilir.
   3. Özellik Mühendisliği: Unsupervised learning sürecinde elde edilen yapılar ve desenler, yeni özellikler oluşturmak için kullanılabilir. Bu yeni özellikler, supervised learning modellerinin performansını artırabilir.
   4. Model Eğitimi: Elde edilen etiketli veri seti kullanılarak supervised learning modelleri (örneğin, karar ağaçları, lojistik regresyon) eğitilir.
   5. Model Değerlendirme ve İyileştirme: Eğitilen modellerin performansı değerlendirilir ve gerekirse hiperparametre ayarlamaları veya model iyileştirmeleri yapılır.

   Bu süreç, unsupervised learning ile elde edilen bilgilerin supervised learning modellerine entegre edilmesini sağlar ve daha etkili tahminler yapılmasına olanak tanır."

18. K-Means kümeleme algoritması nasıl çalışır?

   "K-Means kümeleme algoritması, verileri K adet kümeye ayırmak için kullanılan popüler bir denetimsiz öğrenme yöntemidir.
   Algoritmanın çalışma prensibi şu şekildedir:
   1. K Değerinin Belirlenmesi: Kullanıcı tarafından belirlenen K sayısı, oluşturulacak küme sayısını ifade eder.
   2. Başlangıç Merkezlerinin Seçimi: Rastgele olarak K adet başlangıç küme merkezi (centroid) seçilir.
   3. Küme Atama: Her veri noktası, en yakın küme merkezine atanır. Bu, genellikle Öklidyen mesafe kullanılarak yapılır.
   4. Merkezlerin Güncellenmesi: Her küme için yeni merkezler, o kümedeki tüm veri noktalarının ortalaması alınarak hesaplanır.

