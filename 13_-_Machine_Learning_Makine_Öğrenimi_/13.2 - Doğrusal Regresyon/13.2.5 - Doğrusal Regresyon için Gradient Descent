BAŞLIK: DOĞRUSAL REGRESYON İÇİN GRADYAN İNİŞ (GRADIENT DESCENT)
TÜR: Optimizasyon Algoritması ve Matematiksel Uygulama

================================================================================
1. GİRİŞ: NEDEN İHTİYACIMIZ VAR?
================================================================================
Doğrusal Regresyonun formülü şuydu:
y = w*x + b

Burada 'x' ve 'y' sabittir (veridir). Amacımız en iyi 'w' (eğim) ve 'b' (kesim noktası) değerlerini bulmaktır.

"En iyi" ne demek? Hatanın (MSE - Ortalama Kare Hata) EN AZ olduğu nokta demek.
İşte Gradient Descent, bu hatayı minimum yapan w ve b değerlerini deneme-yanılma (iterasyon) yoluyla bulan algoritmadır.

================================================================================
2. MALİYET FONKSİYONU VE "KASE" ŞEKLİ (THE BOWL)
================================================================================
Doğrusal Regresyonda hata fonksiyonu (Cost Function - J), parametrelere (w, b) göre çizildiğinde 3 boyutlu bir "Kase" (Bowl) şeklini alır.

- Teknik Terim: Dışbükey Fonksiyon (Convex Function).
- Önemi: Bu şekil harikadır çünkü sadece TEK BİR dip noktası (Global Minimum) vardır. Çukurlar, tepeler yoktur.
- Sonuç: Gradient Descent algoritması, doğru ayarlanırsa Doğrusal Regresyon için HER ZAMAN en iyi sonucu bulur. Bir çukura takılıp kalma riski yoktur.

================================================================================
3. ALGORİTMANIN ÇALIŞMA MANTIĞI (ADIM ADIM)
================================================================================

Adım 1: Başlatma (Initialization)
w ve b için rastgele başlangıç değerleri seçilir (Genelde 0 veya küçük rastgele sayılar).
Örn: w=0, b=0.

Adım 2: Tahmin ve Hata (Forward Pass)
Mevcut w ve b ile tahmin yapılır ve Hata (Cost) hesaplanır.
Hata = (Gerçek - Tahmin)^2

Adım 3: Gradyan (Türev) Hesabı
Burası işin matematiğidir. "Hata fonksiyonunun eğimi ne tarafa?" sorusu sorulur.
Matematiksel olarak, Cost fonksiyonunun w ve b'ye göre "Kısmi Türevi" alınır.

- Eğer eğim pozitifse: W değerini azaltmalıyız (Sağdayız, sola git).
- Eğer eğim negatifse: W değerini artırmalıyız (Soldayız, sağa git).

Adım 4: Güncelleme (Update Rule)
Parametreler şu formülle güncellenir:

w_yeni = w_eski - (Öğrenme Oranı * w_gradyanı)
b_yeni = b_eski - (Öğrenme Oranı * b_gradyanı)

Adım 5: Tekrar (Iteration)
Bu işlem hata sıfıra yaklaşana veya adım sayısı bitene kadar (Epoch) binlerce kez tekrarlanır.

================================================================================
4. MATEMATİKSEL FORMÜLLER (ARKA PLAN)
================================================================================
Doğrusal Regresyon için Gradyan formülleri şöyledir (Basitleştirilmiş türev):

Tahmin: ŷ = wx + b
Hata (J): (1/n) * Σ(y - ŷ)^2

1. Ağırlık (w) için Gradyan (Türev):
   dw = (-2/n) * Σ(x * (y - ŷ))
   (Anlamı: Hata ile x değerinin çarpımının ortalaması)

2. Sapma (b) için Gradyan (Türev):
   db = (-2/n) * Σ(y - ŷ)
   (Anlamı: Sadece hatanın ortalaması)

Güncelleme Kodu (Python mantığı):
w = w - (learning_rate * dw)
b = b - (learning_rate * db)

================================================================================
5. ÖĞRENME ORANI (LEARNING RATE / ALPHA) ETKİSİ
================================================================================
Güncelleme formülündeki o çarpan hayati önem taşır.

A. Çok Küçük Alpha (Örn: 0.000001)
- "Karınca adımları."
- Sonuç: Minimum noktaya ulaşır ama bu çok uzun sürer (milyonlarca adım). İşlemciyi yorar.

B. Çok Büyük Alpha (Örn: 0.9)
- "Dev adımları."
- Sonuç: Minimum noktayı (dibi) ıskalar. Karşı yamaca savrulur. Hata azalacağına artabilir (Divergence).

C. Uygun Alpha (Örn: 0.01)
- Başlangıçta büyük adımlar atar, dibe yaklaştıkça eğim azaldığı için adımlar otomatik olarak küçülür ve tam merkeze oturur.

================================================================================
6. ÖZET SENARYO: MODEL NASIL AKILLANIR?
================================================================================
Senaryo: Ev fiyatı tahmini (m2'ye göre).
Gerçek: 100 m2 = 500.000 TL.

1. Başlangıç: w=0, b=0.
   Tahmin: 0 TL.
   Hata: Çok büyük (500.000^2).
   Gradyan: "W değerini acilen ve çok miktarda artır!" der.

2. 10. Adım: w=1000, b=100.
   Tahmin: 100.100 TL.
   Hata: Hala büyük ama azaldı.
   Gradyan: "W değerini artırmaya devam et ama biraz daha sakin."

3. 1000. Adım: w=5000, b=0.
   Tahmin: 500.000 TL.
   Hata: 0.
   Gradyan: 0 (Eğim bitti, düzlükteyiz).
   Güncelleme: Artık w değişmez. Model eğitilmiştir.

================================================================================
SONUÇ VE İPUÇLARI
================================================================================
- Özellik Ölçeklendirme (Feature Scaling): Gradient Descent kullanıyorsanız, verilerinizi (X) mutlaka 0-1 arasına sıkıştırın (Standardization/Normalization). Yoksa algoritma çok yavaş çalışır veya sapıtır.
- Stokastik Gradyan İniş (SGD): Veri seti çok büyükse, her adımda tüm evleri hesaplamak yerine rastgele tek bir evi seçerek güncelleme yapmak (SGD) çok daha hızlıdır.