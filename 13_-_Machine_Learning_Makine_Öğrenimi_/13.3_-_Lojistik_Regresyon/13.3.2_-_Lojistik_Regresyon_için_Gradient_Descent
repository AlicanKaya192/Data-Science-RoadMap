BAŞLIK: LOJİSTİK REGRESYON İÇİN GRADYAN İNİŞ (GRADIENT DESCENT)
TÜR: Optimizasyon Algoritması ve Matematiksel Derinlik

================================================================================
1. GİRİŞ: TEMEL PROBLEM
================================================================================
Doğrusal Regresyon bölümünde Gradyan İniş'in nasıl çalıştığını (tepeden aşağı inen dağcı) görmüştük. Lojistik Regresyon'da mantık aynıdır ancak **arazi (fonksiyonun şekli)** ve **kullanılan harita (formül)** tamamen değişir.

- Amaç: Modelin tahmin ettiği olasılık ($\hat{y}$) ile gerçek sınıf etiketi ($y$) arasındaki hatayı en aza indiren $w$ (Ağırlık) ve $b$ (Sapma) değerlerini bulmaktır.

================================================================================
2. KRİTİK SORU: NEDEN "MSE" KULLANMIYORUZ?
================================================================================
Doğrusal Regresyon'da "Hataların Karesini" (MSE) alıyorduk ve mükemmel bir "Kase" (Convex) şekli elde ediyorduk.

Ancak Lojistik Regresyon'da formülün içinde **Sigmoid Fonksiyonu** ($1 / 1+e^{-z}$) vardır.
Eğer Sigmoid içeren bir denklemin karesini alırsanız, ortaya çıkan grafik "Kase" şeklinde olmaz; **dalgalı, girintili çıkıntılı (Non-Convex)** bir şekil olur.

- Tehlike: Gradyan İniş algoritması bu dalgaların birinde (Local Minimum) takılı kalır ve asla en dibe (Global Minimum - En iyi çözüm) ulaşamaz.
- Çözüm: Bu yüzden MSE yerine **Log Loss (Cross Entropy)** kullanılır.

================================================================================
3. MALİYET FONKSİYONU: LOG LOSS (CROSS ENTROPY)
================================================================================
Matematikçiler, Sigmoid fonksiyonunun yarattığı o dalgalanmayı düzeltmek için Logaritma kullanırlar. Bu, grafiği tekrar pürüzsüz bir kaseye dönüştürür.

Tek Bir Örnek İçin Kayıp Fonksiyonu:
$L(\hat{y}, y) = - [y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y})]$

- Mantık:
  * Eğer $y=1$ ise: Sadece ilk terim çalışır ($- \log(\hat{y})$). Model 1'e yaklaşırsa hata 0 olur.
  * Eğer $y=0$ ise: Sadece ikinci terim çalışır ($- \log(1-\hat{y})$). Model 0'a yaklaşırsa hata 0 olur.

Tüm Veri Seti İçin Maliyet (Cost) Fonksiyonu ($J$):
Tüm örneklerin kayıplarının ortalamasıdır. Gradyan İniş'in minimize etmeye çalıştığı asıl fonksiyon budur.

================================================================================
4. TÜREVLER VE GÜNCELLEME KURALI (ŞAŞIRTICI BENZERLİK)
================================================================================
Maliyet fonksiyonumuz (Log Loss) Doğrusal Regresyon'dan (MSE) tamamen farklı olmasına rağmen, türevini (Calculus ile) aldığımızda ortaya çıkan **güncelleme formülü** şaşırtıcı bir şekilde Doğrusal Regresyon ile AYNI görünür.

Gradyan Formülleri:
1. $w$ için Türev ($dw$):
   $dw = \frac{1}{m} \sum (\text{Tahmin} - \text{Gerçek}) \cdot x$

2. $b$ için Türev ($db$):
   $db = \frac{1}{m} \sum (\text{Tahmin} - \text{Gerçek})$

GÜNCELLEME ADIMI:
$w_{yeni} = w_{eski} - (\alpha \cdot dw)$
$b_{yeni} = b_{eski} - (\alpha \cdot db)$

*** DİKKAT (NUANS): ***
Formüller aynı görünse de **Tahmin** kısmının içi farklıdır!
- Doğrusal Regresyonda Tahmin: $wx + b$
- Lojistik Regresyonda Tahmin: $\text{Sigmoid}(wx + b)$

Bu yüzden hesaplanan sayısal değerler ve algoritmanın davranışı tamamen farklıdır.

================================================================================
5. ADIM ADIM ÇALIŞMA MEKANİZMASI
================================================================================

Adım 1: Başlatma
Ağırlıkları ($w$) ve sapmayı ($b$) sıfıra veya rastgele küçük sayılara eşitle.

Adım 2: İleri Yayılım (Forward Propagation)
- Lineer işlem yap: $z = w \cdot x + b$
- Aktivasyon (Sigmoid) uygula: $\hat{y} = \sigma(z)$
- Bu, bize 0 ile 1 arasında bir olasılık verir.

Adım 3: Maliyet Hesabı
- Log Loss formülü ile mevcut hatayı hesapla. (Amacımız bunu sıfıra yaklaştırmak).

Adım 4: Geriye Yayılım (Backward Propagation / Gradient Calculation)
- Hatayı azaltmak için $w$ ve $b$'yi ne yönde değiştirmeliyiz?
- Türevleri ($dw, db$) hesapla.
  $dw = x \cdot (\hat{y} - y)$ mantığıyla hesaplanır.

Adım 5: Parametre Güncelleme
- Öğrenme Oranı ($\alpha$) kadar ters yöne adım at.
- $w = w - \alpha \cdot dw$

Adım 6: Döngü
- Adım 2'ye dön ve hata minimum olana kadar (veya Epoch sayısı bitene kadar) tekrarla.

================================================================================
6. VEKTÖRİZASYON (NEDEN "FOR" DÖNGÜSÜ KULLANMIYORUZ?)
================================================================================
Lojistik Regresyon genellikle büyük veri setlerinde kullanılır (Örn: 1 milyon veri).
Eğer yukarıdaki işlemleri her bir veri satırı için tek tek `for` döngüsüyle yaparsanız (Stochastic Gradient Descent haricinde), işlem saatler sürer.

Çözüm: Vektörizasyon (NumPy vb.)
Tüm veri setini ($X$) devasa bir matris olarak alıp, ağırlık vektörü ($W$) ile tek seferde çarparız.
- $Z = \text{np.dot}(W^T, X) + b$
- $A = \text{sigmoid}(Z)$
- Bu işlem, modern işlemcilerin (CPU/GPU) paralel işlem gücünü kullanarak saniyeler içinde biter.

================================================================================
ÖZET TABLO
================================================================================
BİLEŞEN             | DOĞRUSAL REGRESYON            | LOJİSTİK REGRESYON
------------------- | ----------------------------- | -----------------------------
Tahmin Fonksiyonu   | $h(x) = wx + b$               | $h(x) = \sigma(wx + b)$
Maliyet Fonksiyonu  | $(y - \hat{y})^2$ (MSE)       | $-[y\log(\hat{y}) + \dots]$
Fonksiyon Şekli     | Kase (Convex)                 | Log sayesinde Kase (Convex)
Türev Mantığı       | Fark $\times$ Girdi           | Fark $\times$ Girdi (Aynı)*
Amaç                | Sayısal Hata Minimizasyonu    | Olasılık Hata Minimizasyonu

*Not: Türev formülü yazım olarak aynı olsa da, işlenen "Tahmin" değeri farklı olduğu için matematiksel sonuç farklıdır.