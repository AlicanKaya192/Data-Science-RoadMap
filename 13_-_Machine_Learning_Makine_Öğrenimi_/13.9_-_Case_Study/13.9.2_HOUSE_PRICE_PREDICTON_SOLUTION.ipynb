{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acabf519",
   "metadata": {},
   "source": [
    "# House Price Prediction Project\n",
    "\n",
    "This notebook is an end-to-end machine learning project developed to predict house prices. It includes Exploratory Data Analysis (EDA), Feature Engineering, Data Preprocessing, and prediction steps using various regression models.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Requirements](#1-requirements)\n",
    "2. [Data Overview](#2-data-overview)\n",
    "3. [Feature Engineering](#3-feature-engineering)\n",
    "4. [Modeling](#4-modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f79c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. REQUIREMENTS\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc10262",
   "metadata": {},
   "source": [
    "## TASK 1 : Apply EDA to the Dataset\n",
    "\n",
    "1. General Picture\n",
    "2. Analysis of Categorical Variables\n",
    "3. Analysis of Numerical Variables\n",
    "4. Analysis of Target Variable\n",
    "5. Analysis of Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe318d2",
   "metadata": {},
   "source": [
    "### Loading and Merging the Dataset\n",
    "Train and Test datasets are read. To ensure consistency in Feature Engineering operations, these two datasets are combined and processed on a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read and merge Train and Test datasets. Proceed with the merged data.\n",
    "\n",
    "# Combining train and test sets.\n",
    "train = pd.read_csv(\"Datasets ( Genel )/train.csv\")\n",
    "test = pd.read_csv(\"Datasets ( Genel )/test.csv\")\n",
    "df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "# Let's examine the first and last rows of the dataset for initial observations.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b6ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704399ec",
   "metadata": {},
   "source": [
    "### Data Overview\n",
    "The dimensions, variable types, missing values, and basic statistics of the dataset are examined using the `check_df` function. This step is critical for understanding the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5850eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. General Picture\n",
    "\n",
    "def check_df(dataframe):\n",
    "    \"\"\"\n",
    "    Prints the basic summary of the dataframe to the screen.\n",
    "    \"\"\"\n",
    "    print(\"##################### Shape #####################\")\n",
    "    print(dataframe.shape)\n",
    "    print(\"##################### Types #####################\")\n",
    "    print(dataframe.dtypes)\n",
    "    print(\"##################### Head #####################\")\n",
    "    print(dataframe.head(3))\n",
    "    print(\"##################### Tail #####################\")\n",
    "    print(dataframe.tail(3))\n",
    "    print(\"##################### NA #####################\")\n",
    "    print(dataframe.isnull().sum())\n",
    "    print(\"##################### Quantiles #####################\")\n",
    "    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n",
    "\n",
    "\n",
    "check_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767607e",
   "metadata": {},
   "source": [
    "### Grab Column Names\n",
    "Variables in the dataset are categorized as categorical, numerical, and \"categorical but cardinal\" (categorical variables with high cardinality). This separation determines how each variable will be approached in subsequent analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a86adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAPTURING NUMERIC AND CATEGORICAL VARIABLES\n",
    "\n",
    "def grab_col_names(dataframe, cat_th=10, car_th=20):\n",
    "    \"\"\"\n",
    "    Separates categorical and numerical variables in the dataframe with logical thresholds.\n",
    "    \"\"\"\n",
    "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
    "\n",
    "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n",
    "                   dataframe[col].dtypes != \"O\"]\n",
    "\n",
    "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n",
    "                   dataframe[col].dtypes == \"O\"]\n",
    "\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "\n",
    "    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n",
    "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
    "\n",
    "    print(f\"Observations: {dataframe.shape[0]}\")\n",
    "    print(f\"Variables: {dataframe.shape[1]}\")\n",
    "    print(f'cat_cols: {len(cat_cols)}')\n",
    "    print(f'num_cols: {len(num_cols)}')\n",
    "    print(f'cat_but_car: {len(cat_but_car)}')\n",
    "    print(f'num_but_cat: {len(num_but_cat)}')\n",
    "\n",
    "    return cat_cols, cat_but_car, num_cols\n",
    "\n",
    "cat_cols, cat_but_car, num_cols = grab_col_names(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c011b667",
   "metadata": {},
   "source": [
    "### Categorical Variable Analysis\n",
    "The classes of categorical variables and their ratios in the dataset are examined. Visualization is performed using the `cat_summary` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd6b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Analysis of Categorical Variables\n",
    "\n",
    "def cat_summary(dataframe, col_name, plot=False):\n",
    "    \"\"\"\n",
    "    Reports class counts and ratios for a categorical variable.\n",
    "    \"\"\"\n",
    "    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n",
    "                        \"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)}))\n",
    "\n",
    "    if plot:\n",
    "        sns.countplot(x=dataframe[col_name], data=dataframe)\n",
    "        plt.show(block=True)\n",
    "\n",
    "\n",
    "for col in cat_cols:\n",
    "    cat_summary(df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed472c33",
   "metadata": {},
   "source": [
    "### Numerical Variable Analysis\n",
    "Distributions of numerical variables are examined using histograms and basic statistics (mean, standard deviation, quartiles). The `num_summary` function performs this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c02bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Analysis of Numerical Variables\n",
    "\n",
    "def num_summary(dataframe, numerical_col, plot=False):\n",
    "    \"\"\"\n",
    "    Provides summary statistics and optionally a histogram for a numerical variable.\n",
    "    \"\"\"\n",
    "    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n",
    "    print(dataframe[numerical_col].describe(quantiles).T)\n",
    "\n",
    "    if plot:\n",
    "        dataframe[numerical_col].hist(bins=50)\n",
    "        plt.xlabel(numerical_col)\n",
    "        plt.title(numerical_col)\n",
    "        plt.show(block=True)\n",
    "\n",
    "    print(\"#####################################\")\n",
    "\n",
    "\n",
    "for col in num_cols:\n",
    "    num_summary(df, col, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68df34",
   "metadata": {},
   "source": [
    "### Target Analysis\n",
    "The effect of categorical variables on the target variable (SalePrice) is examined. By calculating the average house price for each categorical class, we try to understand which classes are determinants of the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c33f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Analysis of Target Variable\n",
    "\n",
    "def target_summary_with_cat(dataframe, target, categorical_col):\n",
    "    \"\"\"\n",
    "    Reports the mean of the target variable according to the classes of a categorical variable.\n",
    "    \"\"\"\n",
    "    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "for col in cat_cols:\n",
    "    target_summary_with_cat(df,\"SalePrice\",col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4567c83",
   "metadata": {},
   "source": [
    "### Target Variable Distribution\n",
    "The distribution of the target variable (SalePrice) is examined. Since it shows a right-skewed distribution, the effect of logarithmic transformation is also observed. Log transformation can improve model performance by approximating the distribution to normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7dfa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMATION\n",
    "# Examination of the dependent variable\n",
    "df[\"SalePrice\"].hist(bins=100)\n",
    "plt.show(block=True)\n",
    "\n",
    "# Examination of the logarithm of the dependent variable\n",
    "np.log1p(df['SalePrice']).hist(bins=50)\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e6a31",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "A correlation matrix is created to examine the relationship between numerical variables and visualized using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b2a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Analysis of Correlation\n",
    "\n",
    "corr = df[num_cols].corr()\n",
    "# Displaying correlations\n",
    "sns.set(rc={'figure.figsize': (12, 12)})\n",
    "sns.heatmap(corr, cmap=\"RdBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b83626",
   "metadata": {},
   "source": [
    "### Detection of High Correlation Variables\n",
    "Variables with very high correlation with each other (multicollinearity) can cause noise and overfitting in the model. These variables are identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c447af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_correlated_cols(dataframe, plot=False, corr_th=0.70):\n",
    "    \"\"\"\n",
    "    Detects columns with high correlation and optionally plots a heatmap.\n",
    "    \"\"\"\n",
    "    corr = dataframe.corr()\n",
    "    cor_matrix = corr.abs()\n",
    "    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))  # np.bool yerine bool\n",
    "    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n",
    "    if plot:\n",
    "        import seaborn as sns\n",
    "        import matplotlib.pyplot as plt\n",
    "        sns.set(rc={'figure.figsize': (15, 15)})\n",
    "        sns.heatmap(corr, cmap=\"RdBu\")\n",
    "        plt.show()\n",
    "    return drop_list\n",
    "\n",
    "high_correlated_cols(df, plot=False)\n",
    "high_correlated_cols(df, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82021f6d",
   "metadata": {},
   "source": [
    "## Task 2 : Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b730b76",
   "metadata": {},
   "source": [
    "### Outlier Analysis\n",
    "Outliers in the dataset can distort the model's generalization ability. Lower and upper threshold values are determined using the IQR (Interquartile Range) method, and values outside these limits are considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83756e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Analysis\n",
    "\n",
    "# Suppression of outliers\n",
    "def outlier_thresholds(dataframe, variable, low_quantile=0.10, up_quantile=0.90):\n",
    "    \"\"\"\n",
    "    Calculates lower and upper threshold values (outlier limits) for a variable.\n",
    "    \"\"\"\n",
    "    quantile_one = dataframe[variable].quantile(low_quantile)\n",
    "    quantile_three = dataframe[variable].quantile(up_quantile)\n",
    "    interquantile_range = quantile_three - quantile_one\n",
    "    up_limit = quantile_three + 1.5 * interquantile_range\n",
    "    low_limit = quantile_one - 1.5 * interquantile_range\n",
    "    return low_limit, up_limit\n",
    "\n",
    "# Outlier check\n",
    "def check_outlier(dataframe, col_name):\n",
    "    \"\"\"\n",
    "    Checks if there are outliers (outside calculated thresholds) in the specified column.\n",
    "    \"\"\"\n",
    "    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n",
    "    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "for col in num_cols:\n",
    "    if col != \"SalePrice\":\n",
    "      print(col, check_outlier(df, col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f862feaa",
   "metadata": {},
   "source": [
    "### Outlier Suppression (Winsorization)\n",
    "Detected outliers are suppressed to the calculated lower and upper threshold values. This reduces the impact of outliers without data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression of outliers\n",
    "def replace_with_thresholds(dataframe, variable):\n",
    "    \"\"\"\n",
    "    Clips outliers to the calculated lower/upper threshold values for the relevant variable.\n",
    "    \"\"\"\n",
    "    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n",
    "    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n",
    "    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n",
    "\n",
    "\n",
    "for col in num_cols:\n",
    "    if col != \"SalePrice\":\n",
    "        replace_with_thresholds(df,col)\n",
    "\n",
    "for col in num_cols:\n",
    "    if col != \"SalePrice\":\n",
    "      print(col, check_outlier(df, col))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e534d3f",
   "metadata": {},
   "source": [
    "### Missing Value Analysis\n",
    "Missing values in the dataset are examined. The number of missing values in which variables and their ratios are reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value Analysis\n",
    "\n",
    "def missing_values_table(dataframe, na_name=False):\n",
    "    \"\"\"\n",
    "    Reports columns containing missing values with their counts and ratios.\n",
    "    \"\"\"\n",
    "    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n",
    "\n",
    "    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n",
    "\n",
    "    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n",
    "\n",
    "    print(missing_df, end=\"\\n\")\n",
    "\n",
    "    if na_name:\n",
    "        return na_columns\n",
    "\n",
    "missing_values_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576fcc7",
   "metadata": {},
   "source": [
    "### Missing Value Imputation (Special Cases)\n",
    "Missing values (NA) in some variables actually indicate that the house does not have that feature (e.g., if there is no pool, PoolQC might be NA). Such missing values are filled with a meaningful label like \"No\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac91ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Alley\"].value_counts()\n",
    "\n",
    "# Missing values in some variables indicate that the house does not have that feature\n",
    "no_cols = [\"Alley\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"FireplaceQu\",\n",
    "           \"GarageType\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\"PoolQC\",\"Fence\",\"MiscFeature\"]\n",
    "\n",
    "# Filling missing values in columns with \"No\"\n",
    "for col in no_cols:\n",
    "    df[col].fillna(\"No\", inplace=True)\n",
    "\n",
    "missing_values_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38605ac",
   "metadata": {},
   "source": [
    "### Missing Value Imputation with Median/Mode\n",
    "For the remaining missing values; categorical variables are filled with the mode (most frequent value), and numerical variables are filled with the median. This process is automated with the `quick_missing_imp` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c75c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function ensures that missing values are filled with median or mean\n",
    "def quick_missing_imp(data, num_method=\"median\", cat_length=20, target=\"SalePrice\"):\n",
    "    \"\"\"\n",
    "    Fills missing values quickly.\n",
    "    \"\"\"\n",
    "    variables_with_na = [col for col in data.columns if data[col].isnull().sum() > 0]  # Variables with missing values are listed\n",
    "\n",
    "    temp_target = data[target]\n",
    "\n",
    "    print(\"# BEFORE\")\n",
    "    print(data[variables_with_na].isnull().sum(), \"\\n\\n\")  # Number of missing values of variables before application\n",
    "\n",
    "    # If variable is object and class count is less than or equal to cat_length, fill missing values with mode\n",
    "    data = data.apply(lambda x: x.fillna(x.mode()[0]) if (x.dtype == \"O\" and len(x.unique()) <= cat_length) else x, axis=0)\n",
    "\n",
    "    # If num_method is mean, missing values of non-object variables are filled with mean\n",
    "    if num_method == \"mean\":\n",
    "        data = data.apply(lambda x: x.fillna(x.mean()) if x.dtype != \"O\" else x, axis=0)\n",
    "    # If num_method is median, missing values of non-object variables are filled with median\n",
    "    elif num_method == \"median\":\n",
    "        data = data.apply(lambda x: x.fillna(x.median()) if x.dtype != \"O\" else x, axis=0)\n",
    "\n",
    "    data[target] = temp_target\n",
    "\n",
    "    print(\"# AFTER \\n Imputation method is 'MODE' for categorical variables!\")\n",
    "    print(\" Imputation method is '\" + num_method.upper() + \"' for numeric variables! \\n\")\n",
    "    print(data[variables_with_na].isnull().sum(), \"\\n\\n\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "df = quick_missing_imp(df, num_method=\"median\", cat_length=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a897d64",
   "metadata": {},
   "source": [
    "### Rare Analysis\n",
    "Rare classes (those with very low frequency) in categorical variables are analyzed. These classes can often create noise for the model or do not carry enough information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70924cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform rare analysis and apply rare encoder.\n",
    "\n",
    "# Examination of the distribution of categorical columns\n",
    "def rare_analyser(dataframe, target, cat_cols):\n",
    "    \"\"\"\n",
    "    Prints the summary of categorical columns with class count, distribution ratio, and target mean.\n",
    "    \"\"\"\n",
    "    for col in cat_cols:\n",
    "        print(col, \":\", len(dataframe[col].value_counts()))\n",
    "        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n",
    "                            \"RATIO\": dataframe[col].value_counts() / len(dataframe),\n",
    "                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n",
    "\n",
    "rare_analyser(df, \"SalePrice\", cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a1ece",
   "metadata": {},
   "source": [
    "### Rare Encoding\n",
    "Rare classes falling below the determined threshold value are combined under the \"Rare\" label. This process reduces the cardinality of categorical variables and ensures the model works more stably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871823cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection of rare classes\n",
    "def rare_encoder(dataframe, rare_perc):\n",
    "    \"\"\"\n",
    "    Combines rare classes (frequency below `rare_perc`) under the 'Rare' label.\n",
    "    \"\"\"\n",
    "    temp_df = dataframe.copy()\n",
    "\n",
    "    rare_columns = [col for col in temp_df.columns if temp_df[col].dtypes == 'O'\n",
    "                    and (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]\n",
    "\n",
    "    for var in rare_columns:\n",
    "        tmp = temp_df[var].value_counts() / len(temp_df)\n",
    "        rare_labels = tmp[tmp < rare_perc].index\n",
    "        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n",
    "\n",
    "    return temp_df\n",
    "\n",
    "\n",
    "df = rare_encoder(df, 0.01)\n",
    "rare_analyser(df, \"SalePrice\", cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea72f3ad",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "New and meaningful variables are derived using existing variables. This step helps the model capture hidden patterns in the dataset (e.g., total house area, age of the house, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7deb377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variables and add 'NEW' to the beginning of the new variables you create.\n",
    "\n",
    "df[\"NEW_1st*GrLiv\"] = df[\"1stFlrSF\"] * df[\"GrLivArea\"]\n",
    "\n",
    "df[\"NEW_Garage*GrLiv\"] = (df[\"GarageArea\"] * df[\"GrLivArea\"])\n",
    "\n",
    "df[\"TotalQual\"] = df[[\"OverallQual\", \"OverallCond\", \"ExterQual\", \"ExterCond\", \"BsmtCond\", \"BsmtFinType1\",\n",
    "                      \"BsmtFinType2\", \"HeatingQC\", \"KitchenQual\", \"Functional\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"Fence\"]].sum(axis = 1)\n",
    "\n",
    "\n",
    "# Total Floor\n",
    "df[\"NEW_TotalFlrSF\"] = df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]\n",
    "\n",
    "# Total Finished Basement Area\n",
    "df[\"NEW_TotalBsmtFin\"] = df.BsmtFinSF1 + df.BsmtFinSF2\n",
    "\n",
    "# Porch Area\n",
    "df[\"NEW_PorchArea\"] = df.OpenPorchSF + df.EnclosedPorch + df.ScreenPorch + df[\"3SsnPorch\"] + df.WoodDeckSF\n",
    "\n",
    "# Total House Area\n",
    "df[\"NEW_TotalHouseArea\"] = df.NEW_TotalFlrSF + df.TotalBsmtSF\n",
    "\n",
    "df[\"NEW_TotalSqFeet\"] = df.GrLivArea + df.TotalBsmtSF\n",
    "\n",
    "\n",
    "# Lot Ratio\n",
    "df[\"NEW_LotRatio\"] = df.GrLivArea / df.LotArea\n",
    "\n",
    "df[\"NEW_RatioArea\"] = df.NEW_TotalHouseArea / df.LotArea\n",
    "\n",
    "df[\"NEW_GarageLotRatio\"] = df.GarageArea / df.LotArea\n",
    "\n",
    "# MasVnrArea\n",
    "df[\"NEW_MasVnrRatio\"] = df.MasVnrArea / df.NEW_TotalHouseArea\n",
    "\n",
    "# Dif Area\n",
    "df[\"NEW_DifArea\"] = (df.LotArea - df[\"1stFlrSF\"] - df.GarageArea - df.NEW_PorchArea - df.WoodDeckSF)\n",
    "\n",
    "\n",
    "df[\"NEW_OverallGrade\"] = df[\"OverallQual\"] * df[\"OverallCond\"]\n",
    "\n",
    "\n",
    "df[\"NEW_Restoration\"] = df.YearRemodAdd - df.YearBuilt\n",
    "\n",
    "df[\"NEW_HouseAge\"] = df.YrSold - df.YearBuilt\n",
    "\n",
    "df[\"NEW_RestorationAge\"] = df.YrSold - df.YearRemodAdd\n",
    "\n",
    "df[\"NEW_GarageAge\"] = df.GarageYrBlt - df.YearBuilt\n",
    "\n",
    "df[\"NEW_GarageRestorationAge\"] = np.abs(df.GarageYrBlt - df.YearRemodAdd)\n",
    "\n",
    "df[\"NEW_GarageSold\"] = df.YrSold - df.GarageYrBlt\n",
    "\n",
    "\n",
    "drop_list = [\"Street\", \"Alley\", \"LandContour\", \"Utilities\", \"LandSlope\",\"Heating\", \"PoolQC\", \"MiscFeature\",\"Neighborhood\"]\n",
    "\n",
    "# Dropping variables in drop_list\n",
    "df.drop(drop_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53845c67",
   "metadata": {},
   "source": [
    "### Encoding (Label & One-Hot)\n",
    "Categorical variables are converted into a numerical format that machine learning models can understand. Label Encoding is applied for binary variables, and One-Hot Encoding is applied for multi-class variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Label Encoding & One-Hot Encoding operations.\n",
    "\n",
    "cat_cols, cat_but_car, num_cols = grab_col_names(df)\n",
    "\n",
    "def label_encoder(dataframe, binary_col):\n",
    "    \"\"\"\n",
    "    Converts a binary categorical column to 0/1 values.\n",
    "    \"\"\"\n",
    "    labelencoder = LabelEncoder()\n",
    "    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n",
    "    return dataframe\n",
    "\n",
    "binary_cols = [col for col in df.columns if df[col].dtypes == \"O\" and len(df[col].unique()) == 2]\n",
    "\n",
    "for col in binary_cols:\n",
    "    label_encoder(df, col)\n",
    "\n",
    "\n",
    "def one_hot_encoder(dataframe, categorical_cols, drop_first=True):\n",
    "    \"\"\"\n",
    "    Applies One-Hot Encoding for specified categorical columns.\n",
    "    \"\"\"\n",
    "    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n",
    "    return dataframe\n",
    "\n",
    "df = one_hot_encoder(df, cat_cols, drop_first=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6787aec",
   "metadata": {},
   "source": [
    "## MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7684afa",
   "metadata": {},
   "source": [
    "### Establishing Base Models\n",
    "Different regression algorithms (Linear Regression, Ridge, Lasso, GBM, XGBoost, LightGBM, CatBoost, etc.) are trained with default parameters and RMSE (Root Mean Squared Error) scores are compared. This step is done to determine which model is more suitable for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504639fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3: Model Building\n",
    "\n",
    "# Separate Train and Test data. (Values with empty SalePrice variable are test data.)\n",
    "train_df = df[df['SalePrice'].notnull()]\n",
    "test_df = df[df['SalePrice'].isnull()]\n",
    "\n",
    "y = train_df['SalePrice']  # np.log1p(df['SalePrice'])\n",
    "X = train_df.drop([\"Id\", \"SalePrice\"], axis=1)\n",
    "\n",
    "# Build a model with Train data and evaluate model success.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)\n",
    "\n",
    "\n",
    "models = [('LR', LinearRegression()),\n",
    "          (\"Ridge\", Ridge(random_state=12345)),\n",
    "          (\"Lasso\", Lasso(random_state=12345)),\n",
    "          (\"ElasticNet\", ElasticNet(random_state=12345)),\n",
    "          ('KNN', KNeighborsRegressor()),\n",
    "          ('CART', DecisionTreeRegressor(random_state=12345)),\n",
    "          ('RF', RandomForestRegressor(random_state=12345)),\n",
    "          ('SVR', SVR()),\n",
    "          ('GBM', GradientBoostingRegressor(random_state=12345)),\n",
    "          (\"XGBoost\", XGBRegressor(objective='reg:squarederror', random_state=12345)),\n",
    "          (\"LightGBM\", LGBMRegressor(random_state=12345)),\n",
    "          (\"CatBoost\", CatBoostRegressor(verbose=False, random_state=12345))]\n",
    "\n",
    "for name, regressor in models:\n",
    "    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n",
    "    print(f\"RMSE: {round(rmse, 4)} ({name}) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a02509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SalePrice'].mean()\n",
    "df['SalePrice'].std()\n",
    "df[\"SalePrice\"].hist(bins=100)\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf27d596",
   "metadata": {},
   "source": [
    "### Modeling with Log Transformation\n",
    "The model is rebuilt by taking the logarithm of the target variable (SalePrice). Log transformation can improve the prediction success of the model by correcting the distribution of the target variable. The results are evaluated by converting them back to the original scale (inverse log)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66605590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS : Build a model by performing Log transformation and observe RMSE results.\n",
    "# Note: Do not forget to take the inverse of the Log.\n",
    "\n",
    "# Performing Log transformation\n",
    "\n",
    "\n",
    "train_df = df[df['SalePrice'].notnull()]\n",
    "test_df = df[df['SalePrice'].isnull()]\n",
    "\n",
    "# plt.hist(np.log1p(train_df['SalePrice']), bins=100)\n",
    "y = np.log1p(train_df['SalePrice'])\n",
    "X = train_df.drop([\"Id\", \"SalePrice\"], axis=1)\n",
    "\n",
    "# Splitting data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=17)\n",
    "\n",
    "\n",
    "lgbm = LGBMRegressor().fit(X_train, y_train)\n",
    "y_pred = lgbm.predict(X_test)\n",
    "\n",
    "y_pred\n",
    "# Taking the inverse of the LOG transformation performed\n",
    "new_y = np.expm1(y_pred)\n",
    "new_y\n",
    "new_y_test = np.expm1(y_test)\n",
    "new_y_test\n",
    "\n",
    "np.sqrt(mean_squared_error(new_y_test, new_y))\n",
    "\n",
    "# RMSE : 22866.43915128612"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9047b2",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "Hyperparameter optimization is performed for the selected best model (e.g., LightGBM). Using the GridSearchCV method, different parameter combinations are tried to find the parameters that maximize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d20914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform hyperparameter optimizations.\n",
    "\n",
    "lgbm_model = LGBMRegressor(random_state=46)\n",
    "\n",
    "y = np.expm1(y)  # undo logarithmic transformation\n",
    "rmse = np.mean(np.sqrt(-cross_val_score(lgbm_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n",
    "\n",
    "\n",
    "lgbm_params = {\"learning_rate\": [0.01, 0.1],\n",
    "               \"n_estimators\": [500, 1500],\n",
    "               \"colsample_bytree\": [0.5, 0.7, 1]\n",
    "             }\n",
    "\n",
    "lgbm_gs_best = GridSearchCV(lgbm_model,\n",
    "                            lgbm_params,\n",
    "                            cv=5,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=-1).fit(X, y)\n",
    "\n",
    "\n",
    "lgbm_gs_best.best_params_\n",
    "final_model = lgbm_model.set_params(**lgbm_gs_best.best_params_).fit(X, y)\n",
    "\n",
    "print(f\"Ä°lk RMSE: {rmse}\")\n",
    "rmse_new = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n",
    "print(f\"Yeni RMSE: {rmse_new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29370917",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "The variables that the model gives more importance to when making predictions are visualized. This analysis is important for interpreting the model's decisions and detecting unnecessary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe023b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ranking of features using the feature_importance function indicating the importance level of variables.\n",
    "\n",
    "# feature importance\n",
    "def plot_importance(model, features, num=len(X), save=False):\n",
    "    \"\"\"\n",
    "    Visualizes feature importance levels in the model with a bar chart.\n",
    "    \"\"\"\n",
    "\n",
    "    feature_imp = pd.DataFrame({\"Value\": model.feature_importances_, \"Feature\": features.columns})\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.set(font_scale=1)\n",
    "    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False)[0:num])\n",
    "    plt.title(\"Features\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if save:\n",
    "        plt.savefig(\"importances.png\")\n",
    "\n",
    "\n",
    "plot_importance(final_model, X)\n",
    "plot_importance(final_model, X, num=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d82ccd",
   "metadata": {},
   "source": [
    "### Prediction and Submission\n",
    "Predictions are generated for the test dataset using the optimized model. These predictions are saved in a CSV file in a format suitable for uploading to the Kaggle competition (Id, SalePrice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ca531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the empty SalePrice variables in the test dataframe and\n",
    "# Create a dataframe suitable for submission to the Kaggle page. (Id, SalePrice)\n",
    "\n",
    "test_df[\"SalePrice\"]\n",
    "predictions = final_model.predict(test_df.drop([\"Id\",\"SalePrice\"], axis=1))\n",
    "# predictions = np.expm1(predictions)  # to take inverse if it was log\n",
    "dictionary = {\"Id\":test_df.index, \"SalePrice\":predictions}\n",
    "dfSubmission = pd.DataFrame(dictionary)\n",
    "dfSubmission.to_csv(\"housePricePredictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
