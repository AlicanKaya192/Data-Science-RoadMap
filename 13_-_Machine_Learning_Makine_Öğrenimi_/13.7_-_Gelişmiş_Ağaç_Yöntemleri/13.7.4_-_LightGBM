BAŞLIK: LIGHTGBM (LIGHT GRADIENT BOOSTING MACHINE) - KAPSAMLI REHBER
TÜR: Histogram Tabanlı, Yaprak Odaklı (Leaf-wise), Yüksek Hız

================================================================================
1. GİRİŞ: NEDEN "LIGHT" (HAFİF)?
================================================================================
XGBoost mükemmeldir ancak veri boyutu milyonlarca satıra ulaştığında eğitimi saatler sürebilir. LightGBM, bu süreci dakikalara indirmek için tasarlanmıştır.

"Light" kelimesi modelin başarısız olduğu anlamına gelmez; hafıza (RAM) ve işlemci gücünü çok "hafif" kullandığı anlamına gelir.

Temel Fark:
- XGBoost: Mükemmeliyetçidir. Her adımı kılı kırk yararak hesaplar.
- LightGBM: Pragmatiktir. "Yaklaşık" hesaplamalarla (Histogram) "Kesin" sonuca çok yakın sonuçlar üretir ama bunu 10 kat daha hızlı yapar.

================================================================================
2. EN BÜYÜK DEVRİM: LEAF-WISE (YAPRAK ODAKLI) BÜYÜME
================================================================================
Diğer tüm algoritmalar (CART, Random Forest, XGBoost) ağaçları **Level-wise (Seviye Odaklı)** büyütür. LightGBM ise **Leaf-wise (Yaprak Odaklı)** büyütür.

A. Level-wise (XGBoost Yaklaşımı):
Ağacı katman katman büyütür. 1. kat bitmeden 2. kata geçmez. Dengeli bir ağaç kurar.
- Mantık: "Soldaki dalı böldüysem, sağdaki dalı da bölmeliyim ki simetri bozulmasın."
- Sorun: Bazen sağ taraftaki dalda bölünecek önemli bir bilgi yoktur, ama algoritma yine de orayı hesaplamak için vakit harcar.

B. Leaf-wise (LightGBM Yaklaşımı):
Ağacın dengesiyle ilgilenmez. Hangi yaprak en çok hatayı azaltıyorsa SADECE ORAYA odaklanır ve orayı böler.
- Mantık: "Sol taraftaki yaprakta hata çok yüksek, sağ taraf idare eder. Ben sürekli sol taraftan aşağı doğru inip orayı düzelteceğim."
- Sonuç: Ağaç asimetrik olur, bir tarafı çok derinleşir. Ancak hatayı (loss) çok daha hızlı düşürür.

RİSK UYARISI: Leaf-wise büyüme çok hızlı öğrenir ancak küçük veri setlerinde "Overfitting" (Ezberleme) riski çok daha yüksektir.

================================================================================
3. HIZIN SIRRI 1: HISTOGRAM TABANLI ALGORİTMA
================================================================================


Karar ağaçlarının en çok vakit harcadığı iş: "Sayıları Sıralamak"tır.
En iyi bölünme noktasını bulmak için milyonlarca veriyi küçükten büyüğe sıralamak (Sorting) gerekir. Bu çok maliyetlidir.

LightGBM'in Çözümü (Binning/Kovalama):
Sürekli verileri (Örn: Maaş) sıralamak yerine, onları "Kovalara" (Bins) ayırır.
- Gerçek Veri: [2500, 2505, 2510, 8000, 8020...] (Milyonlarca farklı değer).
- Histogram: [Kova 1: Düşük Maaşlar, Kova 2: Yüksek Maaşlar].
- LightGBM veriyi 255 adet kovaya (bin) sıkıştırır.

Sonuç: 1 Milyon satırı taramak yerine sadece 255 kovayı tarar.
Karmaşıklık: O(Veri Sayısı) --> O(Kova Sayısı).

================================================================================
4. HIZIN SIRRI 2: GOSS (GRADIENT-BASED ONE-SIDE SAMPLING)
================================================================================
Bu yöntem, "Hangi verilere daha çok önem vermeliyim?" sorusuna dahiyane bir cevap verir.

Mantık:
- Hata payı (Gradyanı) büyük olan veri, henüz iyi öğrenilmemiş demektir. (Önemli).
- Hata payı küçük olan veri, zaten öğrenilmiştir. (Daha az Önemli).

GOSS Algoritması:
1. Büyük hatalı verilerin hepsini tut (%100).
2. Küçük hatalı verilerin içinden rastgele bir kısmını seç (Örn: %10).
3. Seçilen bu küçük kısmı, sanki verinin tamamıymış gibi ağırlıklandırarak dengeyi kur.

Sonuç: Veri setini küçültür ama bilgi kaybını minimumda tutar.

================================================================================
5. HIZIN SIRRI 3: EFB (EXCLUSIVE FEATURE BUNDLING)
================================================================================
Büyük veri setlerinde genellikle "Seyrek Veri" (Sparse Data) sorunu vardır (Birçok sütun sıfırdır).
Örneğin: "İstanbul'da mı?" ve "Ankara'da mı?" sütunları asla aynı anda 1 olamaz. Bunlar birbirini dışlayan (Exclusive) özelliklerdir.

EFB Yöntemi:
Bu iki sütunu tek bir sütunda birleştirir.
- İstanbul: 0-10 arası değerler.
- Ankara: 11-20 arası değerler.
Böylece sütun sayısı azalır, hız artar.

================================================================================
6. KATEGORİK VERİLERLE DANS
================================================================================
XGBoost (eski sürümleri) kategorik verileri (Kırmızı, Mavi, Yeşil) sayıya çevirmenizi (One-Hot Encoding) isterdi. Bu da sütun sayısını patlatırdı.

LightGBM, kategorik verileri doğal haliyle işleyebilir.
"Fisher's method" benzeri bir yaklaşımla kategorileri, hedef değişkene olan etkilerine göre gruplar.
Örn: {Elma, Armut} -> Grup A, {Muz, Çilek} -> Grup B.

================================================================================
7. KRİTİK PARAMETRELER VE TUNING
================================================================================
LightGBM kullanıyorsanız `max_depth` parametresini unutun. Yeni patron `num_leaves`.

1. `num_leaves`: Modelin karmaşıklığını belirleyen ANA parametredir.
   - Dikkat: Teorik olarak `num_leaves = 2^(max_depth)` ilişkisi vardır. Ancak `num_leaves` değerini bu formülden biraz daha küçük tutmak overfitting'i engeller.
   
2. `min_data_in_leaf`: Aşırı öğrenmeyi engellemek için en etkili fren mekanizmasıdır.
   - Leaf-wise büyüdüğü için çok derinlere inebilir, bu parametre "Bir yaprakta en az 100 kişi yoksa orayı bölme" diyerek bunu engeller.

3. `max_depth`: Sadece çok aşırı derinleşmeyi önlemek için (limit koymak adına) kullanılır.

4. `learning_rate`: Diğer boosting modelleriyle aynıdır (Küçük adımlar iyidir).

================================================================================
8. AVANTAJLAR VE DEZAVANTAJLAR
================================================================================

AVANTAJLAR:
1. İnanılmaz Hız: Büyük veri setlerinde rakiplerinden kat kat hızlıdır.
2. Düşük Bellek (RAM) Kullanımı: Histogram yöntemi sayesinde veriyi çok az yer kaplayacak şekilde saklar.
3. Ölçeklenebilirlik: Milyonlarca satırlık verilerde rahatlıkla çalışır.
4. GPU Desteği: Ekran kartı ile eğitimi destekler.

DEZAVANTAJLAR:
1. Küçük Veride Riskli: 10.000 satırdan az veriniz varsa LightGBM kullanmayın. Leaf-wise yapısı nedeniyle veriyi çok çabuk ezberler (overfit).
2. Kararsızlık: Parametre ayarları (tuning) XGBoost'a göre biraz daha hassastır.

================================================================================
ÖZET TABLO: XGBOOST vs LIGHTGBM
================================================================================
ÖZELLİK             | XGBOOST                   | LIGHTGBM
------------------- | ------------------------- | ------------------------------
Büyüme Stratejisi   | Level-wise (Katman Katman)| Leaf-wise (Derinlemesine)
Veri İşleme         | Sıralama (Sorting)        | Histogram (Kovalama)
Bellek Kullanımı    | Yüksek                    | Düşük
Hız                 | Hızlı                     | Çok Çok Hızlı
Kategorik Veri      | One-Hot Encoding (Genelde)| Yerel Destek (Native)
En İyi Kullanım     | Orta Ölçekli Veri / Yarışma| Büyük Veri / Endüstriyel