BAŞLIK: XGBOOST (EXTREME GRADIENT BOOSTING) - KAPSAMLI REHBER
TÜR: Optimize Edilmiş Boosting, Düzenlileştirme (Regularization), 2. Derece Türev

================================================================================
1. GİRİŞ: NEDEN HERKES XGBOOST KULLANIYOR?
================================================================================
XGBoost, Gradient Boosting algoritmasının hızlandırılmış, optimize edilmiş ve güçlendirilmiş bir versiyonudur.

Farkı Yaratan Nedir?
Standart bir GBM "Hata odaklı" çalışır. XGBoost ise "Hata + Model Karmaşıklığı" odaklı çalışır.
- Hız: Klasik GBM'den 10 kat daha hızlı olabilir.
- Başarı: "Regularization" (Düzenlileştirme) yeteneği sayesinde ezberlemeyi (overfitting) çok daha iyi engeller.

Slogan: "Hız ve Performans için ölçeklenebilir makine öğrenmesi."

================================================================================
2. TEMEL YENİLİK: MATEMATİKSEL "SİHİR" (2. TÜREV - HESSIAN)
================================================================================
Klasik GBM, hatayı azaltmak için sadece "Gradyan" (1. Türev - Eğim) kullanır.
XGBoost ise Taylor Serisi açılımı kullanarak hem **Gradyan (1. Türev)** hem de **Hessian (2. Türev - Eğrilik)** bilgisini kullanır.

Analoji (Dağcı Örneği):
- GBM (Gradyan): "Eğim aşağı doğru, hadi inelim." (Yönü bilir).
- XGBoost (Gradyan + Hessian): "Eğim aşağı doğru VE zemin aniden dikleşiyor, adımını küçült!" (Yönü + Zeminin şeklini bilir).

Sonuç: Hedefe (Minimum Hata) çok daha hızlı ve güvenli ulaşır.

================================================================================
3. DÜZENLİLEŞTİRME (REGULARIZATION) - L1 VE L2
================================================================================
XGBoost'u "Extreme" yapan özelliklerden biri, Yitim Fonksiyonu'nun içine "Cezasızlık Kriteri" eklemesidir.

Amaç Fonksiyonu = (Ne kadar hata yaptık?) + (Ağaç ne kadar karmaşık?)
Obj(Θ) = L(θ) + Ω(Θ)

Ω (Omega) Kısmı:
1. Gamma ($\gamma$): Bir dalın bölünmesi için gereken minimum kazanç eşiği. (Bu eşik geçilmezse dal budanır).
2. Lambda ($\lambda$) - L2: Ağırlıkların karesini cezalandırır (Ridge benzeri).
3. Alpha ($\alpha$) - L1: Ağırlıkların mutlak değerini cezalandırır (Lasso benzeri).

Bu sayede XGBoost, "Hatam azalsın ama ağacım da çorbaya dönmesin, sade kalsın" diyebilir.

================================================================================
4. SISTEM OPTİMİZASYONU: PARALEL ÇALIŞMA EFSANESİ
================================================================================
Soru: "Boosting sıralı bir işlemdir (ağaçlar birbirini bekler), XGBoost nasıl paralel çalışır?"

Cevap: XGBoost ağaçları paralel oluşturmaz! Ağacın İÇİNDEKİ düğümleri (nodes) paralel oluşturur.
- Bir ağaç büyürken "Hangi özellikten bölmeliyim?" diye düşünür.
- XGBoost, veriyi önceden bloklara ayırır ve sıralar.
- "Yaş", "Gelir", "Eğitim" sütunlarını aynı anda (farklı işlemci çekirdeklerinde) tarar ve en iyi bölünmeyi bulur.

================================================================================
5. BUDAMA MANTIĞI (TREE PRUNING)
================================================================================


Klasik Karar Ağacı (CART): Bölünme kazancı eksiye düştüğü an durur.
XGBoost: "Max_depth" parametresine kadar ağacı sonuna kadar büyütür, sonra geriye doğru (bottom-up) budamaya başlar.

Neden?
Bazen bir bölünme (-2) puan kaybettirir ama bir sonraki bölünme (+10) puan kazandırabilir.
CART ilk adımda durur ve gelecekteki +10'u kaçırır. XGBoost sonuna kadar gider, eğer toplam kazanç negatifse o dalı keser.

================================================================================
6. KAYIP VERİ İLE BAŞA ÇIKMA (SPARSITY AWARENESS)
================================================================================
Veri setinde "NaN" (Boş) değerler varsa ne olur?
- Diğer algoritmalar: Hata verir veya ortalama ile doldurmanızı ister.
- XGBoost: Boş değerler için "Varsayılan Bir Yön" öğrenir.

Algoritma, eğitim sırasında boş değerleri önce Sola, sonra Sağa atar. Hangi taraf daha iyi sonuç veriyorsa, "Bu sütunda boş değer gelirse X tarafına at" diye not alır.

================================================================================
7. SAYISAL ÖRNEK: BENZERLİK SKORU (SIMILARITY SCORE)
================================================================================
XGBoost, Gini veya Entropi yerine kendi türettiği "Similarity Score" formülünü kullanır.

Formül: Score = (Gradyanların Toplamı)^2 / (Hessianların Toplamı + Lambda)

Örnek Senaryo (Basitleştirilmiş):
Bir yaprakta 3 kişi var. Hataları (Residuals): [-0.5, 1.5, -0.5]. Lambda = 0 (Basitlik için).

1. Pay (Gradyan Toplamı)^2 = (-0.5 + 1.5 - 0.5)^2 = (0.5)^2 = 0.25
2. Payda (Hessian Toplamı) = (Diyelim ki her veri için 1) = 3
3. Skor = 0.25 / 3 = 0.08

Şimdi bu yaprağı bölmeyi dener. Sol ve Sağ yaprakların yeni skorlarını hesaplar.
Eğer (Sol Skor + Sağ Skor) - (Eski Skor) > Gamma ise bölünmeyi kabul eder.

================================================================================
8. KRİTİK PARAMETRELER (TUNING REHBERİ)
================================================================================
XGBoost bir yarış arabasıdır, çok fazla düğmesi vardır.

Genel Parametreler:
- `booster`: 'gbtree' (Normal), 'gblinear' (Lineer), 'dart' (Dropouts).

Booster Parametreleri:
1. `eta` (learning_rate): Adım büyüklüğü. (Genelde 0.01 - 0.3 arası). Düşük eta = Daha çok ağaç gerekir.
2. `min_child_weight`: Bir yaprakta olması gereken minimum hessian ağırlığı. (Overfitting'i engeller).
3. `max_depth`: Ağacın derinliği. (Genelde 3-10 arası).
4. `gamma`: Bölünme için gereken minimum kazanç. (Ağacı muhafazakar yapar).
5. `subsample`: Her ağaç için verinin % kaçı kullanılsın? (0.5 - 1.0).
6. `colsample_bytree`: Sütunların % kaçı kullanılsın? (Random Forest özelliği).

================================================================================
9. AVANTAJLAR VE DEZAVANTAJLAR
================================================================================

AVANTAJLAR:
1. Hız ve Performans: C++ tabanlıdır, çok hızlıdır.
2. Dahili Regularization: L1 ve L2 sayesinde GBM'den daha dayanıklıdır.
3. Esneklik: Kendi Objective Function'ınızı yazıp entegre edebilirsiniz.
4. Missing Value: Kayıp veriyi otomatik halleder.

DEZAVANTAJLAR:
1. Çok Fazla Parametre: Doğru ayarları bulmak (Hyperparameter Tuning) zaman alır.
2. Aykırı Değerler: GBM tabanlı olduğu için outlier'lara karşı hala hassastır.
3. Yorumlanabilirlik: Random Forest gibi bu da bir "Kara Kutu" modelidir.

================================================================================
ÖZET TABLO: GBM vs XGBOOST
================================================================================
ÖZELLİK             | STANDART GBM              | XGBOOST
------------------- | ------------------------- | ------------------------------
Matematik           | Sadece 1. Türev (Gradient)| 1. ve 2. Türev (Hessian)
Düzenlileştirme     | Yok (veya zayıf)          | L1 ve L2 (Güçlü)
Kayıp Veri          | Ön işleme gerekir         | Otomatik öğrenir
Budama              | Durdurma kuralı (Pre)     | Büyüt ve Buda (Post-Pruning)
Hız                 | Yavaş (Sıralı)            | Çok Hızlı (Sistem Optimizasyonu)
Popülerlik          | Orta                      | Çok Yüksek (Endüstri Standardı)