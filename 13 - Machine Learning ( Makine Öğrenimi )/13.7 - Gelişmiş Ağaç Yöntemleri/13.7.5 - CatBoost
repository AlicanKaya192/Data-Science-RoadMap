BAÅLIK: CATBOOST (CATEGORICAL BOOSTING) - KAPSAMLI REHBER
TÃœR: Yandex Labs, Kategorik Veri UzmanÄ±, Simetrik AÄŸaÃ§lar (Oblivious Trees)

================================================================================
1. GÄ°RÄ°Å: NEDEN CATBOOST?
================================================================================
Makine Ã¶ÄŸrenmesi modelleri matematiktir ve matematikte kelimeler (kedi, kÃ¶pek, kÄ±rmÄ±zÄ±) geÃ§mez, sadece sayÄ±lar geÃ§er.
Eskiden, elimizdeki kategorik verileri (Ã–rn: "Toyota", "BMW") sayÄ±ya Ã§evirmek iÃ§in can Ã§ekiÅŸirdik (One-Hot Encoding). Bu iÅŸlem veri setini ÅŸiÅŸirir ve yavaÅŸlatÄ±rdÄ±.

CatBoost'un Vaadi:
"Veriyi Ã¶n iÅŸlemden geÃ§irmene gerek yok. Bana 'Toyota' diye ver, gerisini ben hallederim."

KÃ¶keni: Rus teknoloji devi Yandex tarafÄ±ndan, arama motoru sonuÃ§larÄ±nÄ± sÄ±ralamak iÃ§in geliÅŸtirilmiÅŸtir.

================================================================================
2. EN BÃœYÃœK DEVRÄ°M: KATEGORÄ°K VERÄ° Ä°ÅLEME (TARGET ENCODING)
================================================================================
CatBoost, kategorik verileri sayÄ±ya Ã§evirirken "Target Encoding" (Hedef OdaklÄ± Kodlama) benzeri bir yÃ¶ntem kullanÄ±r ama bunu Ã§ok daha akÄ±llÄ±ca yapar.

Klasik Sorun:
EÄŸer "Toyota" kelimesini, tÃ¼m Toyota'larÄ±n ortalama fiyatÄ±yla deÄŸiÅŸtirirseniz (Target Mean), model cevabÄ± gÃ¶rmÃ¼ÅŸ olur. Buna "Data Leakage" (Veri SÄ±zÄ±ntÄ±sÄ±) denir. Model ezberler (Overfitting).

CatBoost Ã‡Ã¶zÃ¼mÃ¼ (Ordered Target Statistics):
Veriyi rastgele karÄ±ÅŸtÄ±rÄ±r (PermÃ¼tasyon). Her satÄ±r iÃ§in sadece KENDÄ°NDEN Ã–NCE gelen satÄ±rlarÄ±n istatistiÄŸini kullanÄ±r.

MantÄ±k:
- 1. SatÄ±r (Toyota): HenÃ¼z geÃ§miÅŸ yok. Ortalama = Genel Ortalama.
- 50. SatÄ±r (Toyota): Kendinden Ã¶nceki 49 Toyota'nÄ±n ortalamasÄ±nÄ± alÄ±r.
- 100. SatÄ±r (Toyota): Kendinden Ã¶nceki 99 Toyota'nÄ±n ortalamasÄ±nÄ± alÄ±r.

SonuÃ§: GeleceÄŸi gÃ¶rmeden (SÄ±zÄ±ntÄ± olmadan) kategorik veriyi mÃ¼kemmel bir sayÄ±sal deÄŸere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

================================================================================
3. MÄ°MARÄ° FARK: SÄ°METRÄ°K AÄAÃ‡LAR (OBLIVIOUS TREES)
================================================================================


XGBoost ve LightGBM asimetrik (dengesiz) aÄŸaÃ§lar kurabilir. CatBoost ise takÄ±ntÄ±lÄ± derecede dÃ¼zenlidir; "Simetrik AÄŸaÃ§lar" kurar.

Simetrik AÄŸaÃ§ Nedir?
AÄŸacÄ±n aynÄ± derinliÄŸindeki tÃ¼m dÃ¼ÄŸÃ¼mler AYNI soruyla bÃ¶lÃ¼nÃ¼r.
- Seviye 1: Hepsi "YaÅŸ > 20" mi diye sorar.
- Seviye 2: Hepsi "Gelir > 5000" mi diye sorar.

AvantajÄ± Nedir?
1. AÅŸÄ±rÄ± Ã–ÄŸrenmeyi Engeller: YapÄ± Ã§ok katÄ± olduÄŸu iÃ§in modelin veriyi ezberlemesi zordur.
2. Ä°nanÄ±lmaz HÄ±zlÄ± Tahmin (Inference): AÄŸaÃ§ yapÄ±sÄ± Ã§ok dÃ¼zenli olduÄŸu iÃ§in bilgisayar iÅŸlemcisi (CPU Cache) tahmin yaparken veriyi Ã§ok hÄ±zlÄ± iÅŸler. EÄŸitimde yavaÅŸ olabilir ama canlÄ±ya alÄ±ndÄ±ÄŸÄ±nda (Production) Ã§ok hÄ±zlÄ± cevap dÃ¶ner.

================================================================================
4. ORDERED BOOSTING: TAHMÄ°N KAYMASINI (PREDICTION SHIFT) ENGELLEME
================================================================================
Bu, CatBoost'un matematiksel kalbidir.

Sorun:
Standart Gradient Boosting'de, hatayÄ± hesaplarken (Residual) kullandÄ±ÄŸÄ±mÄ±z veri ile aÄŸacÄ± eÄŸittiÄŸimiz veri aynÄ±dÄ±r. Bu, modelin hatayÄ± olduÄŸundan kÃ¼Ã§Ã¼k sanmasÄ±na yol aÃ§ar (Bias).

CatBoost Ã‡Ã¶zÃ¼mÃ¼:
Veriyi sanal olarak parÃ§alara ayÄ±rÄ±r.
- Model A'yÄ± verinin ilk yarÄ±sÄ±yla eÄŸitir, ikinci yarÄ±sÄ±ndaki hatayÄ± hesaplar.
- Model B'yi baÅŸka bir parÃ§ayla eÄŸitir, diÄŸerinin hatasÄ±nÄ± hesaplar.
Bu sayede "Kendi kuyruÄŸunu kovalayan kedi" gibi olmaz; hatayÄ± her zaman "gÃ¶rmediÄŸi" veriler Ã¼zerinden Ã¶lÃ§er.

================================================================================
5. SAYISAL SENARYO: RENKLERÄ° SAYIYA Ã‡EVÄ°RME
================================================================================
BasitleÅŸtirilmiÅŸ "Smoothed Target Encoding" FormÃ¼lÃ¼:
DeÄŸer = (SÄ±nÄ±ftaki Hedef ToplamÄ± + Prior) / (SÄ±nÄ±ftaki Veri SayÄ±sÄ± + 1)

Veri Seti: [KÄ±rmÄ±zÄ±, KÄ±rmÄ±zÄ±, Mavi, KÄ±rmÄ±zÄ±]
Hedefler: [1, 0, 1, 1]
Prior (BaÅŸlangÄ±Ã§ Tahmini): 0.5 olsun.

DÃ¶nÃ¼ÅŸÃ¼m SÃ¼reci (Ordered - SÄ±ralÄ±):
1. SatÄ±r (KÄ±rmÄ±zÄ±): GeÃ§miÅŸte hiÃ§ KÄ±rmÄ±zÄ± yok.
   DeÄŸer = (0 + 0.5) / (0 + 1) = 0.5

2. SatÄ±r (KÄ±rmÄ±zÄ±): GeÃ§miÅŸte 1 tane KÄ±rmÄ±zÄ± var (Hedefi 1).
   DeÄŸer = (1 + 0.5) / (1 + 1) = 0.75

3. SatÄ±r (Mavi): GeÃ§miÅŸte Mavi yok.
   DeÄŸer = 0.5

4. SatÄ±r (KÄ±rmÄ±zÄ±): GeÃ§miÅŸte 2 tane KÄ±rmÄ±zÄ± var (Hedefleri 1 ve 0, Toplam 1).
   DeÄŸer = (1 + 0.5) / (2 + 1) = 0.5

*GÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi, her "KÄ±rmÄ±zÄ±" farklÄ± bir sayÄ±ya dÃ¶nÃ¼ÅŸtÃ¼. Bilgi zenginleÅŸti.*

================================================================================
6. KRÄ°TÄ°K PARAMETRELER (TUNING REHBERÄ°)
================================================================================
CatBoost genellikle "VarsayÄ±lan Ayarlarla" (Default) en iyi Ã§alÄ±ÅŸan modeldir. Yine de bilmeniz gerekenler:

1. `cat_features`: BU ZORUNLUDUR. Modele hangi sÃ¼tunlarÄ±n kategorik olduÄŸunu (indekslerini) sÃ¶ylemelisiniz.
2. `iterations`: AÄŸaÃ§ sayÄ±sÄ± (XGBoost'taki n_estimators). Genelde 1000 ve Ã¼zeri.
3. `learning_rate`: Ã–ÄŸrenme hÄ±zÄ±. CatBoost bunu veri setine gÃ¶re otomatik belirleyebilir.
4. `depth`: AÄŸaÃ§ derinliÄŸi. Simetrik olduÄŸu iÃ§in genelde 6-10 arasÄ± idealdir (XGBoost'tan biraz daha derin olabilir).
5. `border_count`: SayÄ±sal deÄŸiÅŸkenleri bÃ¶lerken kaÃ§ tane "Kova" kullanÄ±lacaÄŸÄ± (LightGBM histogram mantÄ±ÄŸÄ±).

================================================================================
7. AVANTAJLAR VE DEZAVANTAJLAR
================================================================================

AVANTAJLAR:
1. Kategorik Veri KralÄ±: One-Hot Encoding ile uÄŸraÅŸmanÄ±za gerek yoktur, hatta yapmamanÄ±z daha iyidir.
2. Overfitting Direnci: Ordered Boosting ve Simetrik AÄŸaÃ§lar sayesinde ezberlemesi Ã§ok zordur. KÃ¼Ã§Ã¼k veri setlerinde bile harika Ã§alÄ±ÅŸÄ±r.
3. VarsayÄ±lan Performans: Parametre ayarÄ± yapmadan (Hyperparameter Tuning) genellikle en iyi sonucu verir.
4. GÃ¶rsellik: EÄŸitim sÄ±rasÄ±nda harika grafikler Ã§izer (CatBoost Viewer).

DEZAVANTAJLAR:
1. YavaÅŸ EÄŸitim: Ã–zellikle "Ordered" modunda ve Ã§ok fazla kategorik veri varsa, LightGBM'e gÃ¶re eÄŸitimi Ã§ok daha uzun sÃ¼rer.
2. KapalÄ± Kutu: Ã–zelleÅŸtirilmesi (Custom Loss Functions) XGBoost kadar esnek deÄŸildir.

================================================================================
BÃœYÃœK FÄ°NAL: ÃœÃ‡ SÄ°LAHÅÃ–RÃœN KARÅILAÅTIRMASI
================================================================================
Ã–ZELLÄ°K             | XGBOOST                 | LIGHTGBM                | CATBOOST
------------------- | ----------------------- | ----------------------- | -----------------------
Ã‡Ä±kÄ±ÅŸ YÄ±lÄ±          | 2014                    | 2017 (Microsoft)        | 2017 (Yandex)
AÄŸaÃ§ YapÄ±sÄ±         | Level-wise (KatmanlÄ±)   | Leaf-wise (Yaprak)      | Symmetric (Simetrik)
HÄ±z (EÄŸitim)        | HÄ±zlÄ±                   | En HÄ±zlÄ± ğŸš€             | YavaÅŸ (Ama kaliteli)
HÄ±z (Tahmin)        | HÄ±zlÄ±                   | HÄ±zlÄ±                   | En HÄ±zlÄ± (Simetri farkÄ±)
Kategorik Veri      | One-Hot (Genelde)       | Ä°yi (Native)            | MÃ¼kemmel (Ä°htisas AlanÄ±)
KullanÄ±cÄ± Dostu     | Orta (Ayar ister)       | Zor (Hassas ayar)       | Kolay (Otomatik)
En Ä°yi Senaryo      | Kaggle YarÄ±ÅŸmalarÄ±      | BÃ¼yÃ¼k Veri (Big Data)   | Kategorik / KarmaÅŸÄ±k Veri