BAŞLIK: AĞIRLIKLARIN BULUNMASI (PARAMETER ESTIMATION)
TÜR: Optimizasyon Algoritmaları ve Matematiksel Süreç

================================================================================
1. GİRİŞ: AĞIRLIK (WEIGHT) NEDİR?
================================================================================
Bir makine öğrenmesi modelinde (örneğin Doğrusal Regresyon veya Yapay Sinir Ağları), öğrenilen bilgi "Ağırlık" (w) ve "Sapma" (b) denilen parametrelerde saklanır.

Denklem: y = (w * x) + b
- x (Girdi): Değiştiremeyiz (Veridir).
- y (Çıktı): Hedefimizdir.
- w ve b (Parametreler): Modelin değiştirebildiği ve en uygununu aradığı ayar düğmeleridir.

"Modeli Eğitmek" demek, hatayı en aza indiren w ve b değerlerini hesaplamak demektir.

================================================================================
2. AMAÇ: YİTİM FONKSİYONUNU (COST FUNCTION) MİNİMİZE ETMEK
================================================================================
Ağırlıkları bulmadan önce, neye göre "iyi" veya "kötü" olduğumuzu tanımlamalıyız.
Buna Yitim Fonksiyonu (Cost Function / Loss Function) denir (Genellikle J(w) ile gösterilir).

Mantık:
1. Rastgele ağırlıklar seç.
2. Tahmin yap.
3. Hatayı ölç (MSE vb.).
4. Bu hatayı MİNİMUM yapan ağırlıkları bulana kadar aramaya devam et.

Matematiksel Hedef: argmin J(w)

================================================================================
3. YÖNTEM 1: ANALİTİK ÇÖZÜM (NORMAL DENKLEM / OLS)
================================================================================
Bu yöntem, iterasyon (döngü) yapmaz. Tek bir matematiksel formülle doğrudan en iyi sonucu hesaplar.

- Yöntem: En Küçük Kareler (Ordinary Least Squares - OLS).
- Matematik: Matris işlemleri kullanır. Türevi sıfıra eşitleyerek tepe/dip noktayı bulur.
- Formül: w = (X^T * X)^-1 * X^T * y

Avantajları:
- Kesin sonuç verir (Yaklaşık değildir, tamdır).
- Küçük veri setlerinde çok hızlıdır.
- "Learning Rate" gibi parametre ayarlarına gerek yoktur.

Dezavantajları:
- Büyük Veride Çöküş: Veri seti çok büyükse (Örn: 1 milyon satır, 1000 sütun), devasa bir matrisin tersini (inverse) almak bilgisayarın belleğini ve işlemcisini kilitler. O(n^3) karmaşıklığına sahiptir.

================================================================================
4. YÖNTEM 2: İTERATİF ÇÖZÜM (GRADYAN İNİŞ - GRADIENT DESCENT)
================================================================================
Modern makine öğrenmesinin ve Derin Öğrenmenin (Deep Learning) temelidir. Tek seferde bulmak yerine, adım adım doğruya yaklaşır.

Analoji (Kör Dağcı):
Gözleri bağlı bir dağcı olduğunuzu ve bir dağın zirvesinden en dibine (Minimum Hata) inmeye çalıştığınızı düşünün.
1. Rastgele bir noktada başlarsın (Rastgele ağırlık atama).
2. Ayağınla eğimi (Gradient) hissedersin.
3. Eğim aşağı ne taraftaysa o yöne bir adım atarsın.
4. Dibe ulaşana kadar bunu tekrar edersin.

Adım Adım Süreç:
1. Başlatma (Initialization): w = 0 veya rastgele bir sayı.
2. İleri Yayılım: Tahmin üret (y_hat).
3. Hata Hesabı: Gerçek değerden farkı bul.
4. Gradyan Hesabı (Türev): Hata fonksiyonunun ağırlığa göre türevini al. Bu bize hatayı azaltmak için ağırlığı arttırmalı mıyız yoksa azaltmalı mıyız, onu söyler.
5. Güncelleme (Update Rule):
   w_yeni = w_eski - (Öğrenme Oranı * Gradyan)

Kritik Kavram: Öğrenme Oranı (Learning Rate / Alpha)
Dağcının adım büyüklüğüdür.
- Çok Büyük Seçilirse: Dibi ıskalayıp karşı tepeye sıçrayabilirsin (Overshoot). Model asla yakınsamaz.
- Çok Küçük Seçilirse: İnmek yıllar sürer (Model çok yavaş eğitilir).

================================================================================
5. GRADYAN İNİŞ TÜRLERİ (VERİYİ NASIL KULLANIYORUZ?)
================================================================================

A. Batch Gradient Descent (Toplu)
- Ağırlıkları güncellemek için verinin TAMAMINI kullanır.
- Avantaj: Kararlı ve güvenli iner.
- Dezavantaj: Veri büyükse her adım çok uzun sürer.

B. Stochastic Gradient Descent (SGD - Stokastik)
- Her adımda sadece rastgele seçilen TEK BİR veri satırını kullanır.
- Avantaj: Çok hızlıdır.
- Dezavantaj: Çok gürültülüdür, zikzak çizerek iner.

C. Mini-Batch Gradient Descent (En Yaygın Olan)
- Ortayoldur. Veriyi küçük paketlere (Batch) böler (Örn: 32'li, 64'lü gruplar).
- Hem hızlıdır hem de SGD'ye göre daha kararlıdır. Derin öğrenmede standarttır.

================================================================================
6. KARŞILAŞTIRMA TABLOSU
================================================================================
ÖZELLİK             | ANALİTİK ÇÖZÜM (OLS)      | GRADYAN İNİŞ (İTERATİF)
------------------- | ------------------------- | --------------------------
Veri Boyutu         | Küçük Veriler İçin        | Büyük Veriler İçin
Hız                 | Anlık (Matris işlemi)     | Yavaş (Epoch sayısı kadar)
Kesinlik            | Tam Kesin Sonuç           | Yaklaşık Sonuç (Yeterince iyi)
Parametre Ayarı     | Yok                       | Var (Learning Rate, Epoch)
Kullanım Alanı      | İstatistik, Basit ML      | Derin Öğrenme, Karmaşık ML

================================================================================
7. YAPAY SİNİR AĞLARINDA DURUM (BACKPROPAGATION)
================================================================================
Yapay sinir ağlarında çok katman olduğu için işler biraz daha karışıktır.
- Yöntem: Geriye Yayılım (Backpropagation).
- Mantık: Zincir Kuralı (Chain Rule) kullanılarak, en sondaki hatanın sorumluluğu geriye doğru katmanlara paylaştırılır.
- "Bu hata senin yüzünden oldu, ağırlığını değiştir" mesajı sondan başa doğru iletilir.